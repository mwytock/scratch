\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epsfig, graphics}
\usepackage{latexsym}
\usepackage{fullpage}
\usepackage[parfill]{parskip}
\usepackage[tight]{subfigure}
\usepackage{hyperref}

\addtolength{\textheight}{.3in}
\pagestyle{empty}

\author{Matt Wytock}
\begin{document}

My previous experiences developing web-scale systems at Google are fundamental to my development as a computer scientist and in motivating my desire to pursue research. Although research at Google takes a somewhat different form than in academia, in particular with different success criteria, I believe there are many similarities between developing new products from scratch and formulating an academic research agenda. Furthermore, the scale at which Google operates has come to define state-of-the-art in distributed systems and having firsthand experience in building such large-scale systems gives me a unique and valuable perspective. In this essay, I describe a project that I cofounded of and worked on from 2008--2010, which has all of these components.

Search advertising is Google's core business. Users come to Google, often many times a day, to search the internet for the most relevant documents pertaining to their information need. Beyond fulfilling users' needs by giving them relevant documents, each query also represents a business opportunity to present users with relevant commercial information. Google's AdWords product exposes this opportunity to third-party advertisers who participate in a real-time auction in order to show their message in a special section of the page. Advertising on Google is big business and as such a huge amount of effort is invested in improving advertising campaigns and the AdWords product that is used to deliver them.

Despite the huge success of AdWords, a major difficulty faced by advertisers is the necessity to maintain huge lists of keywords on which to advertise. Fundamentally, Ad\emph{Words} relies on external advertisers to pick the most relevant keywords for their website and maintain this list as the website changes over time. For large websites this is a monumental task, requiring significant manual effort. Furthermore, it is impossible to predict \emph{a priori} every query that a user may formulate, so inevitably there will be gaps in query stream coverage for any manually chosen set. The end result is less effective advertising campaigns and lost opportunity for Google and advertisers.

Beginning in 2008, I, along with a small number of other senior engineers, started a project that rethinks how this paradigm works. In particular, we focused on completely automating the document to query matching that is currently done manually, thus delivering a new product that is devoid of maintaining massive lists of keywords. Instead of bidding on keywords directly, advertisers associate bids with sections of their website and our product identifies the relevant queries and generates the appropriate ad automatically. This was, and still is, a huge undertaking and thus in order to realize our goals, we relied on many technologies already developed inside the company. For example, Google already has a state-of-the-art information retrieval system and we used many components of this in developing our new product.

My role as a founding member was to scale up the team and deliver an initial prototype of the technology in action. Technically, I had a large role in building all of the key components which included a large-scale data processing pipeline and a new serving system that integrated directly into the Google serving stack. The pipeline was responsible for computing the page to query mapping, joining it with advertiser provided information (e.g. bids) and outputting the data in a form consumed by our serving system. The serving system we also built from scratch to satisfy the tight constraint that return candidate ads on the order of 10s of milliseconds. Even though this was a prototype, we had paying advertisers from a very early stage and thus this was a production-ready system with high uptime requirements.

In addition to the systems work, I was also responsible for measuring and improving the quality of the ads that we delivered. Due to our ads being automatically generated and targeted, many of the existing quality systems were either broken or not applicable and thus we had to develop new ones. One major deficiency was that we could not take advantage of some of the large-scale machine learning systems that improve the quality of Google's ads over time. I led the effort to understand how these systems interacted with our new ads and then developed a novel extension which largely corrected the issue. The net effect was one of the most substantial quality wins the project has seen to date.

My involvement in the project stopped when I enrolled in CMU in 2010. However, since we were able to build a strong team and momentum, the project is continuing strong today. Recently, it graduated from the beta stage, opening up to all advertisers \cite{jain}. Deep changes to a product as successful as AdWords take a long time to take root; however, significant progress has been made and with continued improvements in quality and usability, adoption will continue to increase. More information is available in the first public announcement \cite{gultekin}, as well as the patents that were filed as part of the project \cite{brunsman,brunsman2,brunsman3}.

\renewcommand{\refname}{\vskip -1.5cm}
\footnotesize
\bibliographystyle{plain}
\bibliography{previous}

\end{document}
